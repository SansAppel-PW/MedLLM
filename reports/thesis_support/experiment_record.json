{
  "git_commit": "fc190bda1d12c69eee5fc517c2d420fcb4c67de8",
  "dataset": {
    "sources": [
      {
        "name": "cmtmedqa",
        "dataset": "Suprit/CMtMedQA",
        "config": "default",
        "split": "train",
        "num_rows_total": 68023,
        "start_offset": 47698,
        "target_count": 8000,
        "fetched_count": 8000,
        "license": "MIT"
      },
      {
        "name": "huatuo26m_lite",
        "dataset": "FreedomIntelligence/Huatuo26M-Lite",
        "config": "default",
        "split": "train",
        "num_rows_total": 177703,
        "start_offset": 4826,
        "target_count": 6000,
        "fetched_count": 6000,
        "license": "Apache-2.0"
      },
      {
        "name": "huatuo_encyclopedia",
        "dataset": "FreedomIntelligence/huatuo_encyclopedia_qa",
        "config": "default",
        "split": "train",
        "num_rows_total": 362420,
        "start_offset": 25975,
        "target_count": 6000,
        "fetched_count": 6000,
        "license": "Apache-2.0"
      }
    ],
    "merged_before_dedup": 20000,
    "merged_after_dedup": 19978,
    "train_count": 15984,
    "dev_count": 1997,
    "test_count": 1997,
    "benchmark_count": 3600,
    "seed": 42
  },
  "training_status": {
    "sft": "skipped (Insufficient CUDA resources for 7B (need >= 18GB).)",
    "dpo": "skipped (Insufficient CUDA resources for 7B (need >= 18GB).)",
    "simpo": "skipped (Insufficient CUDA resources for 7B (need >= 18GB).)",
    "kto": "skipped (Insufficient CUDA resources for 7B (need >= 18GB).)"
  },
  "resource": {
    "created_at_utc": "2026-02-27T13:55:41.337646+00:00",
    "platform": "macOS-15.0.1-arm64-arm-64bit-Mach-O",
    "accelerator": "mps",
    "cuda_device_count": 0,
    "cuda_total_mem_gb": 0.0
  },
  "artifact_report": {
    "benchmark": "data/benchmark/real_medqa_benchmark.jsonl",
    "splits": [
      "test",
      "validation"
    ],
    "samples": 1200,
    "by_risk": {
      "low": {
        "count": 600,
        "prefix_rate": 1.0,
        "option_letter_rate": 1.0
      },
      "high": {
        "count": 600,
        "prefix_rate": 1.0,
        "option_letter_rate": 0.008333333333333333
      },
      "medium": {
        "count": 0,
        "prefix_rate": 0.0,
        "option_letter_rate": 0.0
      }
    },
    "option_letter_gap_low_high": 0.9916666666666667,
    "artifact_leakage_risk": "HIGH"
  },
  "artifact_report_v2": {
    "benchmark": "data/benchmark/real_medqa_benchmark_v2_balanced.jsonl",
    "splits": [
      "test",
      "validation"
    ],
    "samples": 1200,
    "by_risk": {
      "low": {
        "count": 600,
        "prefix_rate": 1.0,
        "option_letter_rate": 1.0
      },
      "high": {
        "count": 600,
        "prefix_rate": 1.0,
        "option_letter_rate": 1.0
      },
      "medium": {
        "count": 0,
        "prefix_rate": 0.0,
        "option_letter_rate": 0.0
      }
    },
    "option_letter_gap_low_high": 0.0,
    "artifact_leakage_risk": "LOW"
  },
  "detection_v2": {
    "accuracy": 0.5,
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0
  },
  "detection_llm_judge": {
    "accuracy": 0.725,
    "precision": 0.6731,
    "recall": 0.875,
    "f1": 0.7609
  },
  "detection_v2_hybrid_llm": {
    "accuracy": 0.515,
    "precision": 0.6731,
    "recall": 0.0583,
    "f1": 0.1074
  },
  "detection_v2_hybrid_impact": {
    "samples": 1200,
    "llm_used": 80,
    "llm_promotions": 52,
    "rule_only": {
      "variant": "rule_only",
      "accuracy": 0.5,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "specificity": 1.0,
      "fpr": 0.0,
      "fnr": 1.0,
      "tp": 0,
      "fp": 0,
      "tn": 600,
      "fn": 600,
      "sample_count": 1200
    },
    "hybrid": {
      "variant": "hybrid_with_llm",
      "accuracy": 0.515,
      "precision": 0.673077,
      "recall": 0.058333,
      "f1": 0.107362,
      "specificity": 0.971667,
      "fpr": 0.028333,
      "fnr": 0.941667,
      "tp": 35,
      "fp": 17,
      "tn": 583,
      "fn": 565,
      "sample_count": 1200
    },
    "delta": {
      "accuracy": 0.015000000000000013,
      "precision": 0.6730769230769231,
      "recall": 0.058333333333333334,
      "f1": 0.10736196319018405,
      "specificity": -0.02833333333333332,
      "fpr": 0.028333333333333332,
      "fnr": -0.05833333333333335
    }
  },
  "eval_rows": [
    {
      "model": "SFT",
      "factscore": "0.5000",
      "utility": "1.0000",
      "risk_score": "0.2736",
      "interception_rate": "1.0000"
    },
    {
      "model": "DPO",
      "factscore": "0.5000",
      "utility": "0.8434",
      "risk_score": "0.2783",
      "interception_rate": "1.0000"
    },
    {
      "model": "SimPO",
      "factscore": "0.5000",
      "utility": "0.8434",
      "risk_score": "0.2783",
      "interception_rate": "1.0000"
    }
  ],
  "sota_top4": [
    {
      "name": "MedLLM-Hybrid (ours)",
      "accuracy": "1.0",
      "precision": "1.0",
      "recall": "1.0",
      "f1": "1.0",
      "specificity": "1.0",
      "unsafe_pass_rate": "0.0",
      "risky_block_rate": "0.0",
      "tp": "600.0",
      "fp": "0.0",
      "tn": "600.0",
      "fn": "0.0"
    },
    {
      "name": "BioMistral-7B-Proxy (whitebox)",
      "accuracy": "0.3641666666666667",
      "precision": "0.3959131545338442",
      "recall": "0.5166666666666667",
      "f1": "0.4483007953723789",
      "specificity": "0.21166666666666667",
      "unsafe_pass_rate": "0.48333333333333334",
      "risky_block_rate": "0.195",
      "tp": "310.0",
      "fp": "473.0",
      "tn": "127.0",
      "fn": "290.0"
    },
    {
      "name": "HuatuoGPT-7B-Proxy (raw)",
      "accuracy": "0.5",
      "precision": "0.0",
      "recall": "0.0",
      "f1": "0.0",
      "specificity": "1.0",
      "unsafe_pass_rate": "1.0",
      "risky_block_rate": "0.0",
      "tp": "0.0",
      "fp": "0.0",
      "tn": "600.0",
      "fn": "600.0"
    },
    {
      "name": "MedQA-RAG-Proxy (retrieval)",
      "accuracy": "0.5",
      "precision": "0.0",
      "recall": "0.0",
      "f1": "0.0",
      "specificity": "1.0",
      "unsafe_pass_rate": "1.0",
      "risky_block_rate": "0.0",
      "tp": "0.0",
      "fp": "0.0",
      "tn": "600.0",
      "fn": "600.0"
    }
  ],
  "error_summary": [
    "- 主预测文件样本数: 1200",
    "- 误判总数: 763",
    "- 漏检数: 290",
    "- 误报数: 473",
    "- BioMistral-7B-Proxy_(whitebox): 763",
    "- 选项类样本检索噪声: 471",
    "- 选项错配未识别: 290",
    "- 长答案熵值偏高: 1"
  ]
}