[
  {
    "model": "Med-PaLM 2",
    "method": "Closed-source medical instruction tuning + ensemble refinement",
    "data_setup": "Internal large-scale medical corpora (not fully open)",
    "key_metrics_scope": "USMLE / expert-level QA (paper-reported)",
    "cost_level": "Very High",
    "reproducibility": "Low (closed weights/training data)",
    "local_run_status": "Not runnable locally",
    "audit_evidence": "docs/OPENING_PROPOSAL_EVIDENCE.md",
    "limitations": "Cannot perform fair local retraining/ablation"
  },
  {
    "model": "ChatDoctor",
    "method": "LLaMA-family supervised fine-tuning on medical dialogues",
    "data_setup": "Open dialogue corpora + external medical KB",
    "key_metrics_scope": "Dialogue quality / medical QA correctness",
    "cost_level": "Medium",
    "reproducibility": "Medium (open recipe, environment-sensitive)",
    "local_run_status": "Background baseline (not fully rerun in this repo)",
    "audit_evidence": "docs/OPENING_PROPOSAL_EVIDENCE.md",
    "limitations": "Original training stack/version drift risk"
  },
  {
    "model": "HuatuoGPT / HuatuoGPT-II",
    "method": "Chinese medical adaptation + SFT/alignment",
    "data_setup": "Chinese medical corpora and QA datasets",
    "key_metrics_scope": "Chinese medical QA benchmarks",
    "cost_level": "High",
    "reproducibility": "Medium",
    "local_run_status": "Proxy compared",
    "audit_evidence": "reports/sota_compare.md",
    "limitations": "Current repo comparison is proxy-level, not full official reproduction"
  },
  {
    "model": "DISC-MedLLM",
    "method": "Instruction-tuned Chinese medical LLM baseline",
    "data_setup": "Chinese instruction and medical QA style corpora",
    "key_metrics_scope": "Medical QA / instruction-following",
    "cost_level": "High",
    "reproducibility": "Medium",
    "local_run_status": "Planned (table-level baseline)",
    "audit_evidence": "docs/EXPERIMENT_MASTER_PLAN.md",
    "limitations": "No local full retraining execution yet"
  },
  {
    "model": "Qwen2.5-7B-Instruct (target mainline)",
    "method": "QLoRA/LoRA SFT + alignment (DPO/SimPO planned)",
    "data_setup": "Repo real_* datasets + KG-governed pipeline",
    "key_metrics_scope": "FactScore / safety / utility / ablation",
    "cost_level": "Medium-High",
    "reproducibility": "High (scripts + manifests + guard)",
    "local_run_status": "Blocked by GPU",
    "audit_evidence": "reports/small_real/qwen_layer_b_blocker.md",
    "limitations": "Needs CUDA memory to execute Layer-B full run"
  },
  {
    "model": "TinyGPT2-LoRA (small-real fallback)",
    "method": "Offline LoRA real training fallback path",
    "data_setup": "Current minimal clean split (engineering validation)",
    "key_metrics_scope": "EM / Rouge-L / Char-F1 (small-real sanity metrics)",
    "cost_level": "Low",
    "reproducibility": "High",
    "local_run_status": "Completed",
    "audit_evidence": "reports/small_real/small_real_lora_v9/eval_metrics.json",
    "limitations": "Not thesis main model; only pipeline closure evidence"
  }
]