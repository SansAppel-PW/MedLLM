model,method,data_setup,key_metrics_scope,cost_level,reproducibility,local_run_status,audit_evidence,limitations,evidence_layer,comparability
Med-PaLM 2,Closed-source medical instruction tuning + ensemble refinement,Internal large-scale medical corpora (not fully open),USMLE / expert-level QA (paper-reported),Very High,Low (closed weights/training data),Not runnable locally,docs/OPENING_PROPOSAL_EVIDENCE.md,Cannot perform fair local retraining/ablation,proxy-background,background-only
ChatDoctor,LLaMA-family supervised fine-tuning on medical dialogues,Open dialogue corpora + external medical KB,Dialogue quality / medical QA correctness,Medium,"Medium (open recipe, environment-sensitive)",Background baseline (not fully rerun in this repo),docs/OPENING_PROPOSAL_EVIDENCE.md,Original training stack/version drift risk,proxy-background,background-only
HuatuoGPT / HuatuoGPT-II,Chinese medical adaptation + SFT/alignment,Chinese medical corpora and QA datasets,Chinese medical QA benchmarks,High,Medium,Proxy compared,reports/sota_compare.md,"Current repo comparison is proxy-level, not full official reproduction",proxy-background,background-only
DISC-MedLLM,Instruction-tuned Chinese medical LLM baseline,Chinese instruction and medical QA style corpora,Medical QA / instruction-following,High,Medium,Planned (table-level baseline),docs/EXPERIMENT_MASTER_PLAN.md,No local full retraining execution yet,proxy-background,background-only
