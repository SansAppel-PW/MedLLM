# 项目现状审计报告（2026-02-26）

## 1. 开题约束提取（按 docx > pdf > pptx）

### 1.1 实验目标
- 主目标：构建“数据治理（事前）- 实时检测（事中）- 偏好对齐（事后）”的中文医疗问答幻觉治理闭环。
- 论文核心问题：降低高隐蔽性医疗幻觉（实体混淆、剂量错误、禁忌冲突），并在多轮长上下文中保持事实一致性。

### 1.2 真实训练方法与阶段结构
- 训练主线：`SFT -> DPO/SimPO/KTO`，偏好对齐阶段重点使用“对抗性实体替换”构造 hard negatives。
- 三层结构：
  1) 模拟/流程验证层；
  2) 小规模真实训练层（LoRA/QLoRA 闭环）；
  3) 完整规模实验层（用于论文主结论）。

### 1.3 Baseline 与模型约束
- 基座优先：Qwen 系列（开题中出现 Qwen2.5-7B/14B 与更大规模口径）。
- 对比对象：Med-PaLM、ChatDoctor、HuatuoGPT、DISC-MedLLM（至少方法学或同口径评测对比）。

### 1.4 评估指标约束
- 事实性：FactScore。
- 安全性：幻觉拦截率、逻辑一致性（KG 冲突率/通过率）。
- 通用可用性：Rouge-L / BLEU。
- 对抗性评估：LLM-as-a-Judge Win Rate。
- 必须包含消融：
  - 有/无 KG 清洗；
  - 白盒 vs 黑盒 vs 混合检测；
  - SFT vs DPO vs SimPO（可扩展 KTO）。

## 2. 当前工程落地审计

### 2.1 已具备能力
- 数据清洗、KG 校验、检测模块、评测脚本与报告资产基本齐全。
- 已有真实 SFT 入口：`src/train/real_sft_train.py`（真实 forward/backward、loss、checkpoint、manifest）。
- 三层实验文档已存在：`docs/EXPERIMENT_MASTER_PLAN.md`。

### 2.2 与论文主链不一致或阻塞点
1. 对齐训练（DPO/SimPO/KTO）当前仍是 proxy/simulation，不满足“真实训练证据链”要求。
2. 评测模块尚未实现 `dotenv + OpenAI` 的 LLM-as-a-Judge 自动化调用。
3. `save_total_limit` 当前默认值为 3（开题执行约束要求 2）。
4. 对比实验当前含代理口径报告，需明确“官方基线结果 vs 同口径复现实验”边界。

## 3. 关键疑问（需尽快统一论文口径）
1. 模型规模口径不一致：文档里既有 Qwen2.5-7B/14B，也出现 32B/235B。建议论文主线锁定 7B/14B，32B/235B 仅做引用对照。
2. Baseline 可复现实验边界：Med-PaLM 属闭源，需明确为“文献参考上界”，避免误写为可复现实验。
3. API Judge 可复现策略：需要保留请求配置、模型版本、抽样与缓存，以保证答辩可追溯。

## 4. 结论
- 项目已具备“从数据到评测”的工程骨架，但离“可直接支撑高水平硕士论文”的关键缺口在于：
  - 真实偏好对齐训练未闭环；
  - API Judge 未接入；
  - 训练/评测参数规范尚需完全对齐开题约束。
