{"recorded_at_utc": "2026-02-26T04:41:19.060939+00:00", "run_tag": "small_real_lora_v13", "iteration_generated_at_utc": "2026-02-26T04:38:59.450493+00:00", "small_real_summary": {"train_loss": 10.734590888023376, "final_eval_loss": 10.736875534057617, "exact_match": 0.0, "rouge_l_f1": 0.0, "char_f1": 0.0}, "real_data_summary": {"train_count": 288, "dev_count": 36, "test_count": 36, "benchmark_count": 200}, "real_alignment_summary": {"dpo_pair_count": 288, "dpo_pref_accuracy_after": 0.3715277777777778, "simpo_score_after": 0.816895, "kto_score_after": 0.5875, "best_method": "SimPO", "best_score": 0.816895}, "risk_assessment": [{"type": "技术风险", "level": "中", "summary": "对齐训练已支持真实 DPO，但 SimPO/KTO 仍为代理流程。", "mitigation": "保持真实 DPO 持续迭代，并在后续阶段补齐 SimPO/KTO 真实训练入口。"}, {"type": "算力风险", "level": "高", "summary": "当前环境是否具备 CUDA 决定 Qwen7B 主实验能否执行。", "mitigation": "使用 run_layer_b_qwen_autofallback.sh；无 GPU 输出 blocker，有 GPU 自动 OOM 回退。"}, {"type": "数据风险", "level": "低", "summary": "real_* 数据集已构建且可用于真实训练与评测。", "mitigation": "继续提升数据规模与多源覆盖，并记录许可与偏差分析。"}, {"type": "论文逻辑风险", "level": "中", "summary": "proxy 与 real 结果混写会导致证据链不可信。", "mitigation": "继续保持分层目录与报告口径隔离。"}], "next_min_loop": ["在 GPU 环境运行 Qwen7B Layer-B 自动回退训练脚本并产出真实 loss/ckpt/eval。", "保持真实 DPO，继续扩容偏好对并执行至少 1 组 real-alignment 消融。", "更新 baseline 对比表为“真实主结果 + 代理背景”双层表述。"], "decision": "保持 real-data + real-DPO 路径，下一步优先申请 GPU 推进 Layer-B Qwen7B 主实验。"}
