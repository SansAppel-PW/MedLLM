task: layer_b_qwen7b_qlora
model_name: Qwen/Qwen2.5-7B-Instruct
train_file: data/clean/real_sft_train.jsonl
dev_file: data/clean/real_sft_dev.jsonl
output_dir: checkpoints/layer_b/qwen25_7b_qlora
logging_dir: logs/layer_b/qwen25_7b_qlora
metrics_out: reports/training/layer_b_qwen25_7b_qlora_metrics.json

seed: 42
max_length: 2048
num_train_epochs: 1
max_steps: -1
learning_rate: 2e-5
weight_decay: 0.01
warmup_ratio: 0.03
optim: paged_adamw_8bit

per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: true
num_workers: 2

logging_steps: 10
save_steps: 100
eval_steps: 100
save_total_limit: 3

bf16: true
fp16: false
device_map_auto: true
trust_remote_code: true
local_files_only: false

use_lora: true
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target_modules: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
